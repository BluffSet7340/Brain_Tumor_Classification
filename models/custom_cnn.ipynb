{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7691909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import AlexNet_Weights\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860f6a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset class distribution: Counter({0: 1501, 1: 1501, 2: 1501, 3: 1501})\n",
      "Training set class distribution: Counter({1: 1051, 3: 1051, 0: 1050, 2: 1050})\n",
      "Validation set class distribution: Counter({1: 150, 2: 150, 0: 150, 3: 150})\n",
      "Test set class distribution: Counter({2: 301, 0: 301, 1: 300, 3: 300})\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset using ImageFolder\n",
    "dataset_root = \"../dataset/Augmented/\"  \n",
    "dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "# First split: train (70%) and temp (30%)\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.3,\n",
    "    stratify=dataset.targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: temp into validation (10% of total) and test (20% of total)\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices,\n",
    "    test_size=2/3,  # 2/3 of 30% is 20% of the total dataset\n",
    "    stratify=[dataset.targets[i] for i in temp_indices],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "# Print class distribution in the full dataset\n",
    "print(\"Full dataset class distribution:\", Counter(dataset.targets))\n",
    "\n",
    "# Print class distribution in the training set\n",
    "train_labels = [dataset.targets[i] for i in train_indices]\n",
    "print(\"Training set class distribution:\", Counter(train_labels))\n",
    "\n",
    "# Print class distribution in the validation set\n",
    "val_labels = [dataset.targets[i] for i in val_indices]\n",
    "print(\"Validation set class distribution:\", Counter(val_labels))\n",
    "\n",
    "# Print class distribution in the test set\n",
    "test_labels = [dataset.targets[i] for i in test_indices]\n",
    "print(\"Test set class distribution:\", Counter(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099fb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.25, inplace=False)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.25, inplace=False)\n",
      "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout(p=0.25, inplace=False)\n",
      "    (15): Flatten(start_dim=1, end_dim=-1)\n",
      "    (16): Linear(in_features=100352, out_features=512, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Dropout(p=0.5, inplace=False)\n",
      "    (19): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a function for Kaiming weight initialization\n",
    "def initialize_weights(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "\n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "\n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "\n",
    "            # Flattening the tensor\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Fully connected layers\n",
    "            nn.Linear(in_features=128 * 28 * 28, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=512, out_features=4),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Example usage\n",
    "model = CustomCNN()\n",
    "\n",
    "# Apply Kaiming initialization\n",
    "initialize_weights(model)\n",
    "\n",
    "# Print the model to verify\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = CustomCNN().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR every 10 epochs by a factor of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    # Training phase\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total * 100\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct / total * 100\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"Validation loss improved.\")\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # torch.save(model.state_dict(), \"best_model.pth\")  # Save the best model\n",
    "    else:\n",
    "        print(\"Validation loss did not improve.\")\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    print(f\"Learning rate after epoch {epoch+1}: {scheduler.get_last_lr()}\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Load the best model before testing\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss, correct, total = 0.0, 0, 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "\n",
    "# Reverse the class_to_idx mapping to get idx_to_class\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Get probabilities\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Collect all labels, predictions, and probabilities for metrics\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = correct / total * 100\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "# Convert numeric predictions and labels to class names\n",
    "all_labels_names = [idx_to_class[label] for label in all_labels]\n",
    "all_predictions_names = [idx_to_class[pred] for pred in all_predictions]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[f\"True: {label}\" for label in dataset.classes],  # True labels\n",
    "    columns=[f\"Pred: {label}\" for label in dataset.classes]  # Predicted labels\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=dataset.classes))\n",
    "\n",
    "# Calculate ROC and AUC for each class\n",
    "print(\"\\nROC and AUC Metrics:\")\n",
    "for i, class_name in enumerate(dataset.classes):\n",
    "    true_binary = [1 if label == i else 0 for label in all_labels]  # Binary labels for the current class\n",
    "    probabilities = [prob[i] for prob in all_probabilities]  # Probabilities for the current class\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(true_binary, probabilities)\n",
    "    auc_score = roc_auc_score(true_binary, probabilities)\n",
    "    print(f\"Class '{class_name}': AUC = {auc_score:.4f}\")\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {auc_score:.4f})\")\n",
    "\n",
    "# Plot settings\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pred: 512Glioma",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pred: 512Meningioma",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pred: 512Normal",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pred: 512Pituitary",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ab63a3b0-ed18-4e65-a251-1ad77466dd9b",
       "rows": [
        [
         "True: 512Glioma",
         "272",
         "28",
         "1",
         "0"
        ],
        [
         "True: 512Meningioma",
         "3",
         "290",
         "7",
         "0"
        ],
        [
         "True: 512Normal",
         "7",
         "7",
         "287",
         "0"
        ],
        [
         "True: 512Pituitary",
         "17",
         "33",
         "2",
         "248"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred: 512Glioma</th>\n",
       "      <th>Pred: 512Meningioma</th>\n",
       "      <th>Pred: 512Normal</th>\n",
       "      <th>Pred: 512Pituitary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True: 512Glioma</th>\n",
       "      <td>272</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True: 512Meningioma</th>\n",
       "      <td>3</td>\n",
       "      <td>290</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True: 512Normal</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True: 512Pituitary</th>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Pred: 512Glioma  Pred: 512Meningioma  Pred: 512Normal  \\\n",
       "True: 512Glioma                  272                   28                1   \n",
       "True: 512Meningioma                3                  290                7   \n",
       "True: 512Normal                    7                    7              287   \n",
       "True: 512Pituitary                17                   33                2   \n",
       "\n",
       "                     Pred: 512Pituitary  \n",
       "True: 512Glioma                       0  \n",
       "True: 512Meningioma                   0  \n",
       "True: 512Normal                       0  \n",
       "True: 512Pituitary                  248  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71522af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop with validation\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct, total = 0, 0\n",
    "\n",
    "#     # Training phase\n",
    "#     for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#     train_loss = running_loss / len(train_loader)\n",
    "#     train_accuracy = correct / total * 100\n",
    "\n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     val_loss, correct, total = 0.0, 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     val_loss /= len(val_loader)\n",
    "#     val_accuracy = correct / total * 100\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "#           f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "#           f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "# print(\"Training complete!\")\n",
    "\n",
    "# # Evaluate on test set\n",
    "# model.eval()\n",
    "# test_loss, correct, total = 0.0, 0, 0\n",
    "# all_labels = []\n",
    "# all_predictions = []\n",
    "# all_probabilities = []\n",
    "\n",
    "# # Reverse the class_to_idx mapping to get idx_to_class\n",
    "# idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         test_loss += loss.item()\n",
    "#         probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Get probabilities\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#         # Collect all labels, predictions, and probabilities for metrics\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# test_loss /= len(test_loader)\n",
    "# test_accuracy = correct / total * 100\n",
    "\n",
    "# # Convert numeric predictions and labels to class names\n",
    "# all_labels_names = [idx_to_class[label] for label in all_labels]\n",
    "# all_predictions_names = [idx_to_class[pred] for pred in all_predictions]\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# # Create a DataFrame for better visualization\n",
    "# conf_matrix_df = pd.DataFrame(\n",
    "#     conf_matrix,\n",
    "#     index=[f\"True: {label}\" for label in dataset.classes],  # True labels\n",
    "#     columns=[f\"Pred: {label}\" for label in dataset.classes]  # Predicted labels\n",
    "# )\n",
    "\n",
    "# # Print metrics\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(conf_matrix_df)\n",
    "\n",
    "# # Print detailed classification report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_labels, all_predictions, target_names=dataset.classes))\n",
    "\n",
    "# # Calculate ROC and AUC for each class\n",
    "# print(\"\\nROC and AUC Metrics:\")\n",
    "# for i, class_name in enumerate(dataset.classes):\n",
    "#     true_binary = [1 if label == i else 0 for label in all_labels]  # Binary labels for the current class\n",
    "#     probabilities = [prob[i] for prob in all_probabilities]  # Probabilities for the current class\n",
    "\n",
    "#     # Calculate ROC curve and AUC\n",
    "#     fpr, tpr, _ = roc_curve(true_binary, probabilities)\n",
    "#     auc_score = roc_auc_score(true_binary, probabilities)\n",
    "#     print(f\"Class '{class_name}': AUC = {auc_score:.4f}\")\n",
    "\n",
    "#     # Plot ROC curve\n",
    "#     plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {auc_score:.4f})\")\n",
    "\n",
    "# # Plot settings\n",
    "# plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
    "# plt.title(\"ROC Curves\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
